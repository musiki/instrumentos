
# clusters de c√≥mputo dedicados a IA mas importantes, (2025)

| #   | Nombre           | Pa√≠s        | Ciudad           | Propiedad                     | CPU              | GPU                    | FLOPS (Rmax)        | Nodos   | RAM    | Almacenamiento  | Red               | Energ√≠a (MW) | Agua  | Arquitectura clave            | A√±o  | Uso IA  | Algoritmos clave          | Responsable principal                         | Tiempo GPT-3   | Energ√≠a GPT-3 |
| --- | ---------------- | ----------- | ---------------- | ----------------------------- | ---------------- | ---------------------- | ------------------- | ------- | ------ | --------------- | ----------------- | ------------ | ----- | ----------------------------- | ---- | ------- | ------------------------- | --------------------------------------------- | -------------- | ------------- |
| 1   | Oracle OCI AI    | EE.UU.      | Ashburn          | Oracle (privado)              | AMD EPYC         | A100+H100 (~131k GPUs) | ~2.4 zettaFLOPS*    | >8,000  | ?      | VAST, NVMe      | OCI fabric        | ~20+         | alta  | Escalado masivo multi-tenancy | 2024 | S√≠      | IA multimodal, clientes   | Oracle Cloud AI team                          | ~5‚Äì7 d√≠as      | ~1.1 GWh      |
| 2   | xAI Colossus     | EE.UU.      | Memphis          | xAI (privado)                 | ?                | H100 (100k GPUs)       | ~100 exaFLOPS*      | ~12,500 | ?      | ?               | Spectrum-X        | ~50‚Äì100*     | alta  | Liquid-cooled AI training     | 2024 | S√≠      | LLMs (Grok-3)             | xAI / Elon Musk                               | ~3‚Äì5 d√≠as est. | ~0.8 GWh      |
| 2   | DeepSeek PetaHub | China       | Hangzhou         | DeepSeek (privado)            | ARM Neoverse V2  | H100 (8,192 GPUs)      | ~8 exaFLOPS (FP16)* | 1,024   | 8 PB   | 200 PB          | InfiniBand NDR400 | 4.5          | media | NVIDIA DGX H100 SuperPOD      | 2024 | S√≠      | LLMs, c√≥digo              | DeepSeek AI Team                              | ~1 d√≠a         | ~0.5 GWh      |
| 3   | ABCI 3.0         | Jap√≥n       | Kashiwa          | AIST (estatal)                | Intel Xeon       | H200                   | >3 exaFLOPS (FP8)   | ?       | 6 PB   | 50 PB           | InfiniBand 400G   | ~5‚Äì8         | baja  | Ultra-paralelo para IA        | 2025 | S√≠      | IA en japon√©s, industrial | RIKEN + AIST                                  | ~7‚Äì9 d√≠as est. | ~1.3 GWh      |
| 4   | El Capitan       | EE.UU.      | Livermore        | DOE/LLNL (estatal)            | AMD EPYC         | Instinct MI300A        | 1.74 exaFLOPS       | 11,136  | 5.4 PB | NVMe Rabbit     | Slingshot         | 30           | alta  | APU CPU+GPU integrada         | 2024 | S√≠      | Sim multi-f√≠sica, LLM     | Lawrence Livermore Lab                        | ~7 d√≠as        | ~1.5 GWh      |
| 5   | Frontier         | EE.UU.      | Oak Ridge        | DOE/ORNL (estatal)            | AMD EPYC         | Instinct MI250X        | 1.35 exaFLOPS       | 9,408   | 4.9 PB | Lustre          | Slingshot-11      | 20‚Äì25        | alta  | CPU-GPU AMD HPC stack         | 2022 | S√≠      | LLMs, f√≠sica cu√°ntica     | Oak Ridge National Lab                        | ~7‚Äì10 d√≠as     | ~1.5 GWh      |
| 6   | OpenAI Azure     | EE.UU.      | Iowa (no espec.) | Microsoft/OpenAI (privado)    | ?                | A100+H100 (tens of k)  | ~10‚Äì20 exaFLOPS*    | ?       | ?      | ?               | Azure fabric      | ~10‚Äì20*      | alta  | Model/data parallelism        | 2020 | S√≠      | LLMs (GPT-4, ChatGPT)     | Microsoft / OpenAI                            | ~5‚Äì7 d√≠as est. | ~1.0 GWh      |
| 7   | Dojo (Tesla)     | EE.UU.      | Palo Alto        | Tesla (privado)               | D1 Chip (custom) | D1 accelerators        | >1.0 exaFLOPS**     | 10,000+ | 1.2 PB | Exa-scale array | DojoNet           | ~2.5         | baja  | Training custom ASICs         | 2024 | S√≠      | Visi√≥n, autoaprendizaje   | Ganesh Venkataramanan                         | ~5 d√≠as est.   | ~0.9 GWh      |
| 8   | JUPITER          | Alemania    | J√ºlich           | EuroHPC/J√ºlich                | NVIDIA Grace ARM | H100                   | 0.79 exaFLOPS       | >10,000 | 1.2 PB | NVMe            | InfiniBand        | 7‚Äì9          | baja  | ARM CPU + Hopper modular      | 2024 | S√≠      | IA soberana europea       | FZ J√ºlich / EuroHPC                           | ~10 d√≠as       | ~1.2 GWh      |
| 9   | LUMI             | Finlandia   | Kajaani          | EuroHPC consortium            | AMD EPYC         | MI250X                 | 0.38 exaFLOPS       | 3,560   | 2.0 PB | 117 PB          | InfiniBand        | 7.1          | baja  | Energ√≠a 100% hidroel√©ctrica   | 2022 | S√≠      | IA, simulaciones f√≠sicas  | CSC (Finland)                                 | ~15 d√≠as       | ~1.8 GWh      |
| 10  | Isambard-AI      | Reino Unido | Bristol          | UKRI + U. Bristol             | NVIDIA Grace ARM | Grace-Hopper           | ~0.25 exaFLOPS*     | ~5,000  | 1.5 PB | 20 PiB          | InfiniBand        | <5           | baja  | GH200 AI-optimizado           | 2024 | S√≠      | BioIA, simulaciones       | Simon McIntosh-Smith                          | ~12 d√≠as       | ~1.3 GWh      |
| 11  | Leonardo         | Italia      | Bologna          | EuroHPC/CINECA                | AMD EPYC         | A100                   | 0.25 exaFLOPS       | 3,456   | 1.5 PB | 100 PB          | NVIDIA IB         | ~6           | media | M√≥dulos booster + datos       | 2022 | S√≠      | IA italiana y ciencias    | CINECA Bologna                                | ~15 d√≠as       | ~1.7 GWh      |
| 12  | Meta RSC         | EE.UU.      | Menlo Park       | Meta (privado)                | AMD EPYC         | A100 (16k GPUs)        | ~0.22 exaFLOPS*     | >2,000  | 3 PB   | 175 PB          | InfiniBand        | ~10          | alta  | Mega-cluster entrenamiento    | 2022 | S√≠      | LLaMA, imagen+texto       | Meta AI                                       | ~10 d√≠as       | ~1.4 GWh      |
| 13  | Clementina XXI   | Argentina   | Buenos Aires     | CONICET + MinCyT (p√∫blica)    | AMD EPYC         | NVIDIA A100 (8x)       | ~15 PFLOPS*         | ~50     | 24 TB  | ~2 PB           | 100 Gb Ethernet   | <1           | baja  | IA nacional, cl√∫ster modular  | 2023 | S√≠      | Modelos BERT/GPT peque√±os | Polo Cient√≠fico Tecnol√≥gico                   | ~30‚Äì40 d√≠as    | ~0.4 GWh      |
| 14  | Fire-Flyer 2     | China       | Hangzhou         | DeepSeek/High-Flyer (privado) | ?                | H800 (2,048 GPUs)      | ~2 PFLOPS*          | 256     | ?      | ?               | InfiniBand        | ~3‚Äì5*        | baja  | MoE, FP8 mixed-precision      | 2024 | S√≠      | LLMs (DeepSeek-V3, R1)    | Liang Wenfeng / DeepSeek                      | ~5‚Äì7 d√≠as est. | ~1.2 MWh      |
| 15  | Santos Dumont    | Brasil      | Petr√≥polis       | LNCC (p√∫blica)                | Intel Xeon E5    | NVIDIA V100            | ~1.1 PFLOPS         | 360     | 0.7 PB | ~1 PB           | InfiniBand        | 0.8‚Äì1.2      | baja  | Biomedicina, IA, simulaci√≥n   | 2015 | Parcial | IA biom√©dica y clim√°tica  | Laborat√≥rio Nacional de Computa√ß√£o Cient√≠fica | >2 meses       | ~0.5 GWh      |


*Nota*: Los valores marcados con * son estimaciones basadas en datos disponibles o proyecciones. Los valores con ** indican datos preliminares o no confirmados.


*Nota*: Los valores marcados con * son estimaciones basadas en datos disponibles o proyecciones. Los valores con ** indican datos preliminares o no confirmados.

	‚Ä¢	El s√≠mbolo * indica estimaci√≥n basada en rendimiento FP8 (IA optimizada).
	‚Ä¢	‚ÄúTiempo GPT‚Äë3‚Äù es estimado para 175B par√°metros en condiciones √≥ptimas.
	‚Ä¢	La columna ‚ÄúAgua‚Äù refiere a intensidad relativa de refrigeraci√≥n l√≠quida (baja/alta).


> [!tip]  FLOPS
>  son las siglas de ***Floating-Point Operations Per Second*** (operaciones de coma flotante por segundo), 
>  
>  1. una medida del rendimiento de un ordenador, especialmente para tareas que implican c√°lculos num√©ricos (por ejemplo, simulaciones cient√≠ficas o entrenamiento de inteligencia artificial). 
>  2. Cuenta cu√°ntas operaciones aritm√©ticas en coma flotante (como la suma o multiplicaci√≥n de decimales) puede realizar un sistema en un segundo.
> 3. Se utiliza para evaluar superordenadores (por ejemplo, las clasificaciones TOP500) y clusters de IA.
> 4. Un mayor n√∫mero de FLOPS indica una mayor potencia de c√°lculo, crucial para entrenar grandes modelos ling√º√≠sticos como GPT-3 o simular la f√≠sica.
> 5. La precisi√≥n es importante (por ejemplo, FP64 para el trabajo cient√≠fico, FP8 para la IA), lo que afecta a los valores de FLOPS.
> 6. Ejemplo: Un cl√∫ster con 1 petaFLOPS puede realizar 1 cuatrill√≥n de c√°lculos por segundo, mientras que 1 exaFLOPS maneja 1 quintill√≥n, es decir, mil veces m√°s.


Traducci√≥n realizada con la versi√≥n gratuita del traductor DeepL.com


# upon deepseek

| #   | Nombre              | Pa√≠s        | Ciudad           | Propiedad                  | CPU                     | GPU                          | FLOPS (Rmax)             | Nodos    | RAM       | Almacenamiento  | Red               | Energ√≠a (MW) | Agua  | Arquitectura clave              | A√±o  | Uso IA  | Algoritmos clave            | Responsable principal               | Tiempo GPT-3   | Energ√≠a GPT-3 |
| --- | ------------------- | ----------- | ---------------- | -------------------------- | ----------------------- | ---------------------------- | ------------------------ | -------- | --------- | --------------- | ----------------- | ------------ | ----- | ------------------------------- | ---- | ------- | -------------------------- | ----------------------------------- | -------------- | ------------- |
| 1   | Oracle OCI AI       | EE.UU.      | Distribuido      | Oracle (privado)           | AMD EPYC                | A100+H100 (~131k GPUs)       | ~2.4 zettaFLOPS (FP8)*   | >8k      | ?         | VAST, NVMe      | OCI fabric        | ~20+         | alta  | escalado masivo multi-tenancy   | 2024 | S√≠      | IA multimodal              | Oracle Cloud AI team                | ~5‚Äì7 d√≠as      | ~1.1 GWh      |
| 3   | El Capitan          | EE.UU.      | Livermore        | DOE/LLNL (estatal)         | EPYC                    | Instinct MI300A              | 1.74 exaFLOPS            | 11,136   | 5.4 PB    | NVMe Rabbit     | Slingshot         | 30           | alta  | APU CPU+GPU integrada           | 2024 | S√≠      | sim multi-f√≠sica, LLM      | Lawrence Livermore Lab              | ~7 d√≠as        | ~1.5 GWh      |
| 4   | Frontier            | EE.UU.      | Oak Ridge        | DOE/ORNL (estatal)         | EPYC                    | Instinct MI250X              | 1.35 exaFLOPS            | 9,408    | 4.9 PB    | Lustre          | Slingshot-11      | 22           | alta  | CPU-GPU AMD HPC stack           | 2022 | S√≠      | LLMs, f√≠sica cu√°ntica      | Oak Ridge National Lab              | ~8 d√≠as        | ~1.5 GWh      |
| 6   | JUPITER             | Alemania    | J√ºlich           | EuroHPC/J√ºlich             | Grace ARM               | H100                         | 0.79 exaFLOPS            | >10k     | 1.2 PB    | NVMe            | InfiniBand        | 8.5          | baja  | ARM CPU + Hopper modular        | 2024 | S√≠      | IA soberana europea        | FZ J√ºlich / EuroHPC                 | ~10 d√≠as       | ~1.2 GWh      |
| 7   | ABCI 3.0            | Jap√≥n       | Tokio            | AIST (estatal)             | Intel Xeon              | H200                         | >3 exaFLOPS (FP8)        | 1,536    | 6 PB      | 50 PB           | InfiniBand 400G   | 7            | baja  | ultra-paralelo para IA          | 2025 | S√≠      | IA industrial, japon√©s     | RIKEN + AIST                        | ~8 d√≠as est.   | ~1.3 GWh      |
| 8   | LUMI                | Finlandia   | Kajaani          | EuroHPC consortium         | EPYC                    | MI250X                       | 0.38 exaFLOPS            | 3,560    | 2.0 PB    | 117 PB          | Infiniband        | 7.1          | baja  | energ√≠a 100% hidroel√©ctrica     | 2022 | S√≠      | IA, simulaciones f√≠sicas   | CSC (Finland)                       | ~15 d√≠as       | ~1.8 GWh      |
| 9   | Isambard-AI         | Reino Unido | Bristol          | UKRI + U. Bristol          | Grace ARM               | Grace-Hopper                 | ~0.25 exaFLOPS*          | ~5k      | 1.5 PB    | 20 PiB          | InfiniBand        | 4.8          | baja  | GH200 AI-optimizado             | 2024 | S√≠      | BioIA, simulaciones        | Simon McIntosh-Smith                | ~12 d√≠as       | ~1.3 GWh      |
| 10  | Leonardo            | Italia      | Bolonia          | EuroHPC/CINECA             | EPYC                    | A100                         | 0.25 exaFLOPS            | 3,456    | 1.5 PB    | 100 PB          | NVIDIA IB         | 6.5          | media | m√≥dulos booster + datos         | 2022 | S√≠      | IA italiana y ciencias     | CINECA Bologna                      | ~15 d√≠as       | ~1.7 GWh      |
| 11  | Meta RSC            | EE.UU.      | Menlo Park       | Meta (privado)             | EPYC                    | A100 (16k GPUs)              | ~0.22 exaFLOPS*          | >2k      | 3 PB      | 175 PB          | InfiniBand        | 9.5          | alta  | mega-cluster entrenamiento      | 2022 | S√≠      | LLaMA, imagen+texto        | Meta AI                             | ~10 d√≠as       | ~1.4 GWh      |
| 12  | Dojo (Tesla)        | EE.UU.      | Austin           | Tesla (privado)            | D1 Chip (custom)        | D1 accelerators              | >1.0 exaFLOPS (FP16)**   | 10k+     | ?         | Exa-scale array | DojoNet           | 3.0          | baja  | training custom ASICs           | 2024 | S√≠      | visi√≥n, autoaprendizaje    | Ganesh Venkataramanan               | ~5 d√≠as est.   | ~0.9 GWh      |
| 13  | Clementina XXI      | Argentina   | Buenos Aires     | CONICET + MinCyT (p√∫blica) | AMD EPYC                | NVIDIA A100 (8x)             | ~15 PFLOPS*              | ~50+     | 24 TB     | ~2 PB           | 100 Gb Ethernet   | 0.9          | baja  | IA nacional, cl√∫ster modular    | 2023 | S√≠      | modelos BERT/GPT peque√±os  | Polo Cient√≠fico Tecnol√≥gico         | ~30‚Äì40 d√≠as    | ~0.4 GWh      |
| 14  | Santos Dumont       | Brasil      | Petr√≥polis       | LNCC (p√∫blica)             | Intel Xeon Platinum     | NVIDIA V100                  | ~1.1 PFLOPS              | 360      | 0.7 PB    | ~1 PB           | Infiniband        | 1.0          | baja  | biomedicina, IA, simulaci√≥n     | 2023 | Parcial | IA biom√©dica y clim√°tica   | Laborat√≥rio Nacional de Computa√ß√£o  | >2 meses       | ~0.5 GWh      |


## oracle OCI supercluster (distribuido)
![](https://i.imgur.com/0JctOIW.png)

## colossus (US)

![](https://i.imgur.com/Uop9PFw.png)
![](https://i.imgur.com/8XPv02f.png)


## el capitan (US)
![](https://i.imgur.com/PZ2R2IA.png)
## frontier
![](https://i.imgur.com/psEf4vZ.png)
## jupiter (DE)
![](https://i.imgur.com/p8BWqvH.png)
## lumi (FI)
![](https://i.imgur.com/y2SYJvW.png)
## isambard (UK)
![](https://i.imgur.com/WV1pgJy.png)
## clementine XXI (AR)
![](https://i.imgur.com/V0gUlhb.png)

# comparativas 

### Resumen de datos sobre energ√≠a para entrenar GPT-3 (de la tabla)
- **Oracle OCI AI**: ~1.1 GWh total, ~5‚Äì7 d√≠as ‚Üí ~157.14 MWh/d√≠a (1,100,000 kWh √∑ 7).
- **xAI Colossus**: ~0.8 GWh total, ~3‚Äì5 d√≠as ‚Üí ~160 MWh/d√≠a (800,000 kWh √∑ 5).
- **DeepSeek Fire-Flyer 2**: ~1.2 MWh total, ~5‚Äì7 d√≠as ‚Üí ~0.1714 MWh/d√≠a (1,200 kWh √∑ 7).
- **OpenAI Azure**: ~1.0 GWh total, ~5‚Äì7 d√≠as ‚Üí ~142.86 MWh/d√≠a (1,000,000 kWh √∑ 7).
- **Clementina XXI**: ~0.4 GWh total, ~30‚Äì40 d√≠as ‚Üí ~10 MWh/d√≠a (400,000 kWh √∑ 40).
- **Nota**: Estos son promedios diarios basados en la energ√≠a total dividida por la duraci√≥n estimada del entrenamiento, seg√∫n la tabla.

### Consumo el√©ctrico urbano (por mill√≥n de habitantes, de respuesta previa)
- **EE.UU.**: ~32,877 kWh/d√≠a (Nueva York, escalado desde 12,000 kWh/a√±o per c√°pita).
- **China**: ~12,329 kWh/d√≠a (Hangzhou, escalado desde 4,500 kWh/a√±o per c√°pita, promedio China 2023).
- **Argentina**: ~3,288 kWh/d√≠a (Buenos Aires, escalado desde 1,200 kWh/a√±o per c√°pita, aproximado con datos latinoamericanos).

### Cinco casos
1. Para entrenar un modelo GPT-3 desde cero en **EE.UU**. con el cl√∫ster Oracle OCI AI, gastar√≠amos la **misma** energ√≠a que consume diariamente la bulliciosa ciudad de *Ashburn para un mill√≥n de habitantes*‚Äîunos 32,877 kWh
2. En **EE.UU.**, utilizar el cl√∫ster xAI Colossus en Memphis para entrenar un modelo GPT-3 desde cero consumir√≠a aproximadamente ***cinco*** veces la electricidad diaria de un *mill√≥n de memphianos*‚Äî160,000 kWh frente a sus 32,877 kWh.
3. En **China**, emplear el cl√∫ster DeepSeek Fire-Flyer 2 en Hangzhou para entrenar un modelo GPT-3 desde cero requerir√≠a apenas 1,200 kWh *diarios*, una ***m√≠nima fracci√≥n*** de los 12,329 kWh que alimentan a un *mill√≥n de residentes hangzhouneses cada d√≠a.*
4. Para entrenar un modelo GPT-3 desde cero en **EE.UU**. con el cl√∫ster OpenAI Azure en Iowa, usar√≠amos unos 142,860 kWh *diarios*‚Äîm√°s de ***cuatro*** veces los 32,877 kWh que iluminan hogares y granjas de un *mill√≥n de ioweses.*
5. En **Argentina**, entrenar un modelo GPT-3 desde cero en el cl√∫ster Clementina XXI en Buenos Aires demandar√≠a 10,000 kWh *diarios*‚Äîcasi el ***triple*** de los 3,288 kWh que sostienen a un *mill√≥n de porte√±os cada d√≠a.*


# GPU Benchmark Comparison with Processing Time

| GPU                       | Compute Units / Cores | TFLOPS (FP32) | TFLOPS (FP16) | Training Speed (Token/sec) | Memory (GB) | Power Consumption (W) | Performance Tier | Approx. Time per 1MB ¬± 200k tokens (seconds)    |
| ------------------------- | --------------------- | ------------- | ------------- | -------------------------- | ----------- | --------------------- | ---------------- | -------------------------------------------- |
| Apple M1 (Integrated GPU) | 8-core GPU            | 5.2           | 2.6           | 0.9                        | 16          | 70                    | Medium           | 222,                                            |
| NVIDIA RTX 3070           | 5888 CUDA cores       | 20.3          | 8.5           | 4.8                        | 8           | 220                   | High             |                                                 |
| NVIDIA K80                | 2496 CUDA cores       | 8.7           | 4.5           | 2.1                        | 12          | 300                   | Medium                                                             |
| NVIDIA T4                 | 2560 CUDA cores       | 12.5          | 6.1           | 2.4                        | 16          | 70                    | High                                                               |
| NVIDIA P100               | 3584 CUDA cores       | 15.7          | 7.9           | 3.6                        | 16          | 250                   | Very High                                                          |

processing time calculation will be:
$$ \text{Processing Time (seconds)} = \frac{200,000 \text{ tokens}}{\text{Training Speed (Token/sec)}} $$

---

## üìå Explicaci√≥n de cada columna

### **1. GPU**
- El modelo de la GPU que se est√° comparando.
- Incluye GPU integradas (Apple M1) y GPU externas (NVIDIA RTX 3070, K80, T4, P100).

---

### **2. Unidades de c√°lculo / n√∫cleos**
- N√∫mero de n√∫cleos o unidades de c√°lculo disponibles para el procesamiento.
- Las GPU NVIDIA incluyen n√∫cleos CUDA, mientras que la Apple M1 utiliza n√∫cleos integrados optimizados para tareas de GPU.

---

### **3. TFLOPS (FP32)**
- Teraflops para operaciones en coma flotante de 32 bits (precisi√≥n est√°ndar).
- Los valores m√°s altos indican un mejor rendimiento para entrenar redes neuronales est√°ndar.

---

### **4. TFLOPS (FP16)**
- Teraflops para operaciones en coma flotante de 16 bits (media precisi√≥n).
- Importante para la formaci√≥n de precisi√≥n mixta, que es m√°s r√°pida y consume menos memoria.

---

### **5. Velocidad de entrenamiento (Token/seg)**
- Velocidad estimada de entrenamiento, medida en tokens procesados por segundo.
- Basado en modelos de transformador t√≠picos (por ejemplo, GPT-2, BERT).
- Los valores m√°s altos indican un procesamiento m√°s r√°pido.

---

### **6. Memoria (GB)**
- Cantidad de memoria disponible en la GPU.
- Importante para almacenar los pesos del modelo, los gradientes y grandes lotes de datos durante el entrenamiento.

---

### **7. Consumo de energ√≠a (W)**
- Consumo de energ√≠a aproximado en vatios.
- Un mayor consumo de energ√≠a suele indicar un mejor rendimiento, pero con un mayor coste energ√©tico.

---

### **8. Nivel de rendimiento**
- Una clasificaci√≥n general del rendimiento de la GPU basada en sus especificaciones y la eficiencia esperada.
- Niveles: **Medio**, **Alto**, **Muy alto**, **Extremadamente alto**.

---

### **9. Tiempo aproximado por 1 MB (segundos)**
- Tiempo estimado para procesar **1 MB de datos de texto** (unos 200.000 tokens).
- Calculado como:
  ```markdown
  Tiempo de procesamiento (segundos) = 200.000 / Velocidad de entrenamiento (Token/seg)