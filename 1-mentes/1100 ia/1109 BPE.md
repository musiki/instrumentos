#concept #ai 


<grid drag="60 55" drop="5 10" bg="black" align="left">
## BPE
#### aplicado en ia-bert-text
</grid>

---

#### üîç **¬øQu√© es BPE (Byte Pair Encoding)?**
**Byte Pair Encoding (BPE)** es un m√©todo de **compresi√≥n de datos y tokenizaci√≥n** ampliamente usado en modelos de lenguaje como GPT-2 para descomponer palabras en subpalabras o tokens m√°s peque√±os. 

---
### üîç **¬øC√≥mo Funciona BPE?**
1. **Inicializaci√≥n:** Se parte de un vocabulario que contiene solo caracteres individuales.
2. **Conteo de Frecuencias:** Encuentra las parejas de caracteres adyacentes con mayor frecuencia.
3. **Creaci√≥n de un Nuevo Token:** Reemplaza la pareja m√°s frecuente por un nuevo token.
4. **Repetici√≥n:** El proceso se repite hasta alcanzar el vocabulario deseado.

---
### üîç **F√≥rmula Matem√°tica del BPE:**
El proceso de BPE se puede expresar con la siguiente f√≥rmula:
<math display="block">
<br>
<br>
  <mrow>
    <mo>argmax</mo><mo>(</mo><mi>x</mi>, <mi>y</mi><mo>)</mo>
    <mo>=</mo>
    <msup>
      <mo>‚àë</mo>
      <mrow>
        <mi>i</mi><mo>=</mo><mn>1</mn>
      </mrow>
    </msup>
    <mi>freq</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub>, <msub><mi>y</mi><mi>i</mi></msub><mo>)</mo>
  </mrow>
</math>

---
### üìå **C√≥digo Python para Tokenizaci√≥n (BPE):**
```python
from transformers import GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# Ejemplo de texto
texto = "El r√≠o de palabras se desbord√≥ en la noche..."

tokens = tokenizer.encode(texto)
print(f"Tokens: {tokens}")
print(f"Texto Original: {tokenizer.decode(tokens)}")
```

---
### 2. **Forward Pass:**
Durante el Forward Pass, el modelo genera predicciones sobre el siguiente token bas√°ndose en la secuencia anterior usando la arquitectura Transformer.

---
### üîç **F√≥rmula Matem√°tica del Forward Pass:**
El modelo genera la siguiente palabra basada en la secuencia anterior utilizando:



<br>
<br>
<math display="block">
  <mrow>
    <mi>P</mi><mo>(</mo><msub><mi>w</mi><mi>t</mi></msub><mo>|</mo>
    <msub><mi>w</mi><mn>1</mn></msub>, <msub><mi>w</mi><mn>2</mn></msub>, 
    <mo>...</mo>, <msub><mi>w</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>)</mo>
    <mo>=</mo>
    <mi>Softmax</mi><mo>(</mo><msub><mi>W</mi><mi>o</mi></msub> <msub><mi>h</mi><mi>t</mi></msub><mo>)
  </mrow>
</math>

---
### üîç **Descripci√≥n:**

<math display="block">
<br>
  <mrow>
    <mtext>h<sub>t</sub> </mtext><mtext>: Representaci√≥n del token generada por el Transformer</mtext>
  </mrow>
  <br>
  <mrow>
    <mtext>W</mtext><sub>o</sub><mo>:</mo> <mtext>Matriz de pesos que proyecta</mtext> <mtext>h</mtext><sub>t</sub> <mtext>al espacio del vocabulario.</mtext>
  </mrow>
  <mrow>
<br>
    <mtext>Softmax</mtext><mo>:</mo> <mtext>Convierte los valores en probabilidades.</mtext>
  </mrow>
</math>